{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pickle as pkl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TashkeelDataset(Dataset):\n",
    "    def __init__(self, name, path):\n",
    "        self.name = name\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            self.lines = list(tqdm(file, f\"Reading {self.name} Lines\"))\n",
    "        self._load_dicts()\n",
    "        self.tokenized_lines = self._tokenize_lines()\n",
    "        self.embedded_data = self._embedd_lines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embedded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.embedded_data[idx]\n",
    "        return torch.tensor(x).to(device), torch.tensor(y).to(device)\n",
    "    \n",
    "    def _remove_tashkeel(self,data):\n",
    "        #double damma, double fatha, double kasera, damma, fatha, kasera, sukoon, shadd\n",
    "        TASHKEEL_SET = {'ٌ', 'ً', 'ٍ', 'ُ', 'َ', 'ِ', 'ْ', 'ٌّ', 'ّ'}\n",
    "        DIACRITICS_REGEX = re.compile('|'.join(TASHKEEL_SET))\n",
    "        return re.sub(DIACRITICS_REGEX, '', data)\n",
    "    \n",
    "    def _one_hot_encode(self, indices, size):\n",
    "        return [[1 if i == elem else 0 for i in range(size)] for elem in indices]\n",
    "    \n",
    "    def _chunk_text(self, text, chunk_size):\n",
    "        chunks = []\n",
    "        words = re.findall(r'\\S+', text)\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        for word in words:\n",
    "            if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
    "                current_chunk += f\"{word} \"\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = f\"{word} \"\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return list(filter(None, chunks))\n",
    "    \n",
    "    def _tokenize_lines(self):\n",
    "        # Define a pattern to match specific punctuation marks\n",
    "        punctuation_pattern1 = r'([.,:;؛)\\]}»،])'\n",
    "        punctuation_pattern2 = r'([(\\[{«])'\n",
    "        tokenized_lines = []\n",
    "\n",
    "        for line in tqdm(self.lines, f\"Tokenizing {self.name} Lines\"):\n",
    "            # Replace matched punctuation marks with the same followed by a line break\n",
    "            splitted_line = re.sub(punctuation_pattern1, r'\\1\\n', line)\n",
    "            splitted_line = re.sub(punctuation_pattern2, r'\\n\\1', splitted_line)\n",
    "\n",
    "            # Further split the splitted line into substrings based on line breaks\n",
    "            for sub_line in splitted_line.split('\\n'):\n",
    "                cleaned_sub_line = self._remove_tashkeel(sub_line).strip()\n",
    "                if 0 < len(cleaned_sub_line) <= 500:\n",
    "                    tokenized_lines.append(sub_line.strip())\n",
    "\n",
    "                elif len(cleaned_sub_line) > 500:\n",
    "                    tokenized_lines.extend(self._chunk_text(sub_line.strip(), 500))\n",
    "    \n",
    "        return tokenized_lines\n",
    "\n",
    "    def _load_dicts(self):\n",
    "        with open( '../utilities/pickle_files/LETTERS.pickle', 'rb') as file:\n",
    "            self.LETTERS = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/DIACRITICS.pickle', 'rb') as file:\n",
    "            self.DIACRITICS = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/CHAR_TO_ID.pickle', 'rb') as file:\n",
    "            self.CHAR_TO_ID = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/DIACRITIC_TO_ID.pickle', 'rb') as file:\n",
    "            self.DIACRITIC_TO_ID = pkl.load(file)\n",
    "        \n",
    "    def _embedd_lines(self):\n",
    "        inputs_embeddings=[]\n",
    "        for line in tqdm(self.tokenized_lines, f\"Embedding {self.name} Lines\"):\n",
    "            x = [self.CHAR_TO_ID['<SOS>']]\n",
    "            y = [self.DIACRITIC_TO_ID['<SOS>']]\n",
    "\n",
    "            for index, char in enumerate(line):\n",
    "                if char in self.DIACRITICS:\n",
    "                    continue\n",
    "\n",
    "                if char in self.CHAR_TO_ID:\n",
    "                    x.append(self.CHAR_TO_ID[char])\n",
    "                else:\n",
    "                    x.append(self.CHAR_TO_ID['<UNK>'])\n",
    "\n",
    "                if char not in self.LETTERS:\n",
    "                    y.append(self.DIACRITIC_TO_ID[''])\n",
    "\n",
    "                else:\n",
    "                    char_diac = ''\n",
    "                    if index + 1 < len(line) and line[index + 1] in self.DIACRITICS:\n",
    "                        char_diac = line[index + 1]\n",
    "                        if index + 2 < len(line) and line[index + 2] in self.DIACRITICS and char_diac + line[index + 2] in self.DIACRITIC_TO_ID:\n",
    "                            char_diac += line[index + 2]\n",
    "                        elif index + 2 < len(line) and line[index + 2] in self.DIACRITICS and line[index + 2] + char_diac in self.DIACRITIC_TO_ID:\n",
    "                            char_diac = line[index + 2] + char_diac\n",
    "                    y.append(self.DIACRITIC_TO_ID[char_diac])\n",
    "\n",
    "            x.append(self.CHAR_TO_ID['<EOS>'])\n",
    "            y.append(self.DIACRITIC_TO_ID['<EOS>'])\n",
    "            y = self._one_hot_encode(y, len(self.DIACRITIC_TO_ID))\n",
    "            \n",
    "            inputs_embeddings.append((x, y)) \n",
    "            \n",
    "        return inputs_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train dataset Lines: 2it [00:00, 2003.97it/s]\n",
      "Tokenizing train dataset Lines: 100%|██████████| 2/2 [00:00<?, ?it/s]\n",
      "Embedding train dataset Lines: 100%|██████████| 2/2 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TashkeelDataset('train dataset','test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['هَذِهِ تَجْرِبَةٌ لِلتَّشْكِيلِ بِالذَّكَاءِ الِاصْطِناعيِّ', 'هَذِهِ تَجْرِبَةٌ لِلتَّشْكِيلِ بِالذَّكَاءِ الِاصْطِناعيِّ هَذِهِ تَجْرِبَةٌ لِلتَّشْكِيلِ بِالذَّكَاءِ الِاصْطِناعيِّ']\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.tokenized_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3, '\\n': 4, ' ': 5, '!': 6, '\"': 7, '&': 8, \"'\": 9, '(': 10, ')': 11, '*': 12, '+': 13, ',': 14, '-': 15, '.': 16, '/': 17, '0': 18, '1': 19, '2': 20, '3': 21, '4': 22, '5': 23, '6': 24, '7': 25, '8': 26, '9': 27, ':': 28, ';': 29, '=': 30, '[': 31, ']': 32, '_': 33, '`': 34, '{': 35, '}': 36, '~': 37, '«': 38, '»': 39, '،': 40, '؛': 41, '؟': 42, 'ء': 43, 'آ': 44, 'أ': 45, 'ؤ': 46, 'إ': 47, 'ئ': 48, 'ا': 49, 'ب': 50, 'ة': 51, 'ت': 52, 'ث': 53, 'ج': 54, 'ح': 55, 'خ': 56, 'د': 57, 'ذ': 58, 'ر': 59, 'ز': 60, 'س': 61, 'ش': 62, 'ص': 63, 'ض': 64, 'ط': 65, 'ظ': 66, 'ع': 67, 'غ': 68, 'ف': 69, 'ق': 70, 'ك': 71, 'ل': 72, 'م': 73, 'ن': 74, 'ه': 75, 'و': 76, 'ى': 77, 'ي': 78, '٠': 79, '١': 80, '٢': 81, '٤': 82, '\\u200d': 83, '\\u200f': 84, '–': 85, '’': 86, '“': 87, '…': 88, '﴾': 89, '﴿': 90}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.CHAR_TO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 75, 58, 75, 5, 52, 54, 59, 50, 51, 5, 72, 72, 52, 62, 71, 78, 72, 5, 50, 49, 72, 58, 71, 49, 43, 5, 49, 72, 49, 63, 65, 74, 49, 67, 78, 2]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.embedded_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 75, 58, 75, 5, 52, 54, 59, 50, 51, 5, 72, 72, 52, 62, 71, 78, 72, 5, 50, 49, 72, 58, 71, 49, 43, 5, 49, 72, 49, 63, 65, 74, 49, 67, 78, 2], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]]), ([1, 75, 58, 75, 5, 52, 54, 59, 50, 51, 5, 72, 72, 52, 62, 71, 78, 72, 5, 50, 49, 72, 58, 71, 49, 43, 5, 49, 72, 49, 63, 65, 74, 49, 67, 78, 5, 75, 58, 75, 5, 52, 54, 59, 50, 51, 5, 72, 72, 52, 62, 71, 78, 72, 5, 50, 49, 72, 58, 71, 49, 43, 5, 49, 72, 49, 63, 65, 74, 49, 67, 78, 2], [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])]\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.embedded_data)\n",
    "\n",
    "# List of tuples\n",
    "# Each tuple containts one list representing x, and list of lists representing y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Indexing first tuple\n",
    "print(len(train_dataset.embedded_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset.embedded_data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37])\n",
      "torch.Size([37, 19])\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0][0].shape)\n",
    "print(train_dataset[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset.embedded_data[0][0]))\n",
    "print(len(train_dataset.embedded_data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '', 1: 'َ', 2: 'ً', 3: 'ُ', 4: 'ٌ', 5: 'ِ', 6: 'ٍ', 7: 'ْ', 8: 'ّ', 9: 'َّ', 10: 'ًّ', 11: 'ُّ', 12: 'ٌّ', 13: 'ِّ', 14: 'ٍّ', 15: '<PAD>', 16: '<SOS>', 17: '<EOS>', 18: '<N/A>'}\n"
     ]
    }
   ],
   "source": [
    "with open( '../utilities/pickle_files/ID_TO_DIACRITIC.pickle', 'rb') as file:\n",
    "    ID2DIAC = pkl.load(file)\n",
    "\n",
    "print(ID2DIAC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "يَرى\n",
      "يًرى\n",
      "يُرى\n",
      "يٌرى\n",
      "يِرى\n",
      "يٍرى\n",
      "يْرى\n",
      "يّرى\n",
      "يَّرى\n",
      "يًّرى\n",
      "يُّرى\n",
      "يٌّرى\n",
      "يِّرى\n",
      "يٍّرى\n"
     ]
    }
   ],
   "source": [
    "print(f'ي{ID2DIAC[1]}رى')\n",
    "print(f'ي{ID2DIAC[2]}رى')\n",
    "print(f'ي{ID2DIAC[3]}رى')\n",
    "print(f'ي{ID2DIAC[4]}رى')\n",
    "print(f'ي{ID2DIAC[5]}رى')\n",
    "print(f'ي{ID2DIAC[6]}رى')\n",
    "print(f'ي{ID2DIAC[7]}رى')\n",
    "print(f'ي{ID2DIAC[8]}رى')\n",
    "print(f'ي{ID2DIAC[9]}رى')\n",
    "print(f'ي{ID2DIAC[10]}رى')\n",
    "print(f'ي{ID2DIAC[11]}رى')\n",
    "print(f'ي{ID2DIAC[12]}رى')\n",
    "print(f'ي{ID2DIAC[13]}رى')\n",
    "print(f'ي{ID2DIAC[14]}رى')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from numeric values to diacritic names\n",
    "DIAC_NAMES = {\n",
    "    0: 'No diacritic', 1: 'Fatha', 2: 'Fatha Tanwin', 3: 'Damma', 4: 'Damma Tanwin', \n",
    "    5: 'Kasra', 6: 'Kasra Tanwin', 7: 'Sukoon', 8: 'Shadda', 9: 'Shadda Fatha', \n",
    "    10: 'Shadda Fatha Tanwin', 11: 'Shadda Damma', 12: 'Shadda Damma Tanwin', \n",
    "    13: 'Shadda Kasra', 14: 'Shadda Kasra Tanwin', 15: 'PAD', 16: 'SOS', 17: 'EOS', 18: 'N/A'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('../utilities/pickle_files/DIACRITIC_TO_NAME.pickle', 'wb+') as diac2name:\n",
    "    pkl.dump(DIAC_NAMES, diac2name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'No diacritic', 1: 'Fatha', 2: 'Fatha Tanwin', 3: 'Damma', 4: 'Damma Tanwin', 5: 'Kasra', 6: 'Kasra Tanwin', 7: 'Sukoon', 8: 'Shadda', 9: 'Shadda Fatha', 10: 'Shadda Fatha Tanwin', 11: 'Shadda Damma', 12: 'Shadda Damma Tanwin', 13: 'Shadda Kasra', 14: 'Shadda Kasra Tanwin', 15: 'PAD', 16: 'SOS', 17: 'EOS', 18: 'N/A'}\n"
     ]
    }
   ],
   "source": [
    "with open('../utilities/pickle_files/DIACRITIC_TO_NAME.pickle', 'rb') as diac2name:\n",
    "    diac_to_name = pkl.load(diac2name)\n",
    "print(diac_to_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS \n",
      "Fatha \n",
      "Kasra \n",
      "Kasra \n",
      "No diacritic \n",
      "Fatha \n",
      "Sukoon \n",
      "Kasra \n",
      "Fatha \n",
      "Damma Tanwin \n",
      "No diacritic \n",
      "Kasra \n",
      "No diacritic \n",
      "Shadda Fatha \n",
      "Sukoon \n",
      "Kasra \n",
      "No diacritic \n",
      "Kasra \n",
      "No diacritic \n",
      "Kasra \n",
      "No diacritic \n",
      "No diacritic \n",
      "Shadda Fatha \n",
      "Fatha \n",
      "No diacritic \n",
      "Kasra \n",
      "No diacritic \n",
      "No diacritic \n",
      "Kasra \n",
      "No diacritic \n",
      "Sukoon \n",
      "Kasra \n",
      "No diacritic \n",
      "No diacritic \n",
      "No diacritic \n",
      "Shadda Kasra \n",
      "EOS \n"
     ]
    }
   ],
   "source": [
    "for one_hot in train_dataset[0][1]:\n",
    "    print(f'{DIAC_NAMES[one_hot.argmax().item()]} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, x,y in enumerate(train_dataset.embedded_data):\n",
    "#     print(f'{len(x)} token in x: {x}\\n')\n",
    "#     for i,diac in enumerate(y[idx]):\n",
    "#         print(f'y{i}: {diac}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    x_padded = rnn_utils.pad_sequence(x_batch, batch_first=True, padding_value=train_dataset.CHAR_TO_ID['<PAD>'])\n",
    "    y_padded = rnn_utils.pad_sequence(y_batch, batch_first=True, padding_value=train_dataset.DIACRITIC_TO_ID['<PAD>'])\n",
    "    return x_padded, y_padded\n",
    "\n",
    "# Create a DataLoader instance with collate_fn\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
    "# dataloader_test = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class MeshakkelatyModel(nn.Module):\n",
    "    def __init__(self, char_to_id, diacritic_to_id):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=len(char_to_id),\n",
    "            embedding_dim=25,\n",
    "            padding_idx=char_to_id['<PAD>']  \n",
    "        )\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=25,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            dropout=0.5,\n",
    "            batch_first=True  \n",
    "        )\n",
    "        self.linear1 = nn.Linear(2*256, 512)\n",
    "        self.linear2 = nn.Linear(512, len(diacritic_to_id))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = nn.functional.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "meshakkelaty = MeshakkelatyModel(train_dataset.CHAR_TO_ID, train_dataset.DIACRITIC_TO_ID).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(meshakkelaty.parameters())\n",
    "epochs = 10\n",
    "metric = Accuracy(task=\"multiclass\", num_classes=len(train_dataset.DIACRITIC_TO_ID)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 73, 19])\n"
     ]
    }
   ],
   "source": [
    "x,y=next(iter(dataloader_train))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  0,  0,  ...,  1,  0,  0],\n",
      "         [ 0,  1,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [15, 15, 15,  ..., 15, 15, 15],\n",
      "         [15, 15, 15,  ..., 15, 15, 15],\n",
      "         [15, 15, 15,  ..., 15, 15, 15]],\n",
      "\n",
      "        [[ 0,  0,  0,  ...,  1,  0,  0],\n",
      "         [ 0,  1,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         ...,\n",
      "         [ 1,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  0,  0],\n",
      "         [ 0,  0,  0,  ...,  0,  1,  0]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1, 75, 58, 75,  5, 52, 54, 59, 50, 51,  5, 72, 72, 52, 62, 71, 78, 72,\n",
      "          5, 50, 49, 72, 58, 71, 49, 43,  5, 49, 72, 49, 63, 65, 74, 49, 67, 78,\n",
      "          2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0],\n",
      "        [ 1, 75, 58, 75,  5, 52, 54, 59, 50, 51,  5, 72, 72, 52, 62, 71, 78, 72,\n",
      "          5, 50, 49, 72, 58, 71, 49, 43,  5, 49, 72, 49, 63, 65, 74, 49, 67, 78,\n",
      "          5, 75, 58, 75,  5, 52, 54, 59, 50, 51,  5, 72, 72, 52, 62, 71, 78, 72,\n",
      "          5, 50, 49, 72, 58, 71, 49, 43,  5, 49, 72, 49, 63, 65, 74, 49, 67, 78,\n",
      "          2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 73, 19])\n"
     ]
    }
   ],
   "source": [
    "y_pred = meshakkelaty(x)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 73])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.argmax(dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02054794505238533\n"
     ]
    }
   ],
   "source": [
    "train_acc = metric(y_pred.argmax(dim=-1), y.argmax(dim=-1))\n",
    "print(train_acc.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Accuracy: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1/1 [00:00<00:00, 39.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Accuracy: 0.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1/1 [00:00<00:00, 36.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Accuracy: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1/1 [00:00<00:00, 41.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Accuracy: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Accuracy: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1/1 [00:00<00:00, 40.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Accuracy: 0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Accuracy: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1/1 [00:00<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Accuracy: 0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1/1 [00:00<00:00, 33.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Accuracy: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1/1 [00:00<00:00, 38.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Accuracy: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    meshakkelaty.train()\n",
    "    for x_batch, y_batch in tqdm(dataloader_train, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = meshakkelaty(x_batch)\n",
    "        loss = criterion(y_pred, y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc = metric(y_pred.argmax(dim=-1), y_batch.argmax(dim=-1))\n",
    "    train_acc = metric.compute()       \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train Accuracy: {train_acc:.4f}')\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 1/1 [00:00<00:00,  3.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Accuracy: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 1/1 [00:00<00:00, 39.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Accuracy: 0.0103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 1/1 [00:00<00:00, 36.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Accuracy: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 1/1 [00:00<00:00, 41.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Accuracy: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Accuracy: 0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 1/1 [00:00<00:00, 40.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Accuracy: 0.0080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Accuracy: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 1/1 [00:00<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Accuracy: 0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 1/1 [00:00<00:00, 33.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Accuracy: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 1/1 [00:00<00:00, 38.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Accuracy: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    meshakkelaty.train()\n",
    "    for x_batch, y_batch in tqdm(dataloader_train, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = meshakkelaty(x_batch)\n",
    "        loss = criterion(y_pred, y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc = metric(y_pred.argmax(dim=-1), y_batch.argmax(dim=-1))\n",
    "    train_acc = metric.compute()       \n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train Accuracy: {train_acc:.4f}')\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
