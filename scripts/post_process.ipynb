{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import pickle as pkl\n",
    "import pprint as pp\n",
    "import random\n",
    "from nltk import edit_distance, FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../utilities/pickle_files/DIACRITICS.pickle', 'rb') as file:\n",
    "    DIACRITICS = pkl.load(file)\n",
    "\n",
    "#double damma, double fatha, double kasera, damma, fatha, kasera, sukoon, shadd\n",
    "TASHKEEL_SET = {'ٌ', 'ً', 'ٍ', 'ُ', 'َ', 'ِ', 'ْ', 'ٌّ', 'ّ'}\n",
    "DIACRITICS_REGEX = re.compile('|'.join(TASHKEEL_SET))\n",
    "def remove_diacritics(data):\n",
    "    return re.sub(DIACRITICS_REGEX, '', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(file):\n",
    "    '''\n",
    "    Preprocess the data by removing all non arabic characters and diacritics\n",
    "    '''\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read()\n",
    "\n",
    "    pattern = r'\\u000a+' \n",
    "    result = re.sub(pattern, ' ', lines) # remove end lines\n",
    "    pattern = r'[^\\u0621-\\u0655 ]+' \n",
    "    result = re.sub(pattern, '', result) # remove non arabic characters\n",
    "    pattern = r'\\s+'\n",
    "    result = re.sub(pattern, ' ', result) # remove extra spaces\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDictionary(input_file):\n",
    "    '''\n",
    "    This function takes a file path as input and returns\n",
    "    a dictionary of undiacritized words as keys and all possible diacritized words as values\n",
    "    in addition to a unigram frequency distribution of the words in the file\n",
    "    '''\n",
    "    # Dictionary to store undiacritized words as keys\n",
    "    # and diacritized words as values in a set\n",
    "    word_dict = {}\n",
    "\n",
    "    # Split the line into words based on spaces\n",
    "    words = preprocess(input_file).split()\n",
    "\n",
    "    unigram = FreqDist(words)\n",
    "\n",
    "    # Process each word\n",
    "    for word in words:\n",
    "        # Remove diacritics using the custom function\n",
    "        undiacritized_word = remove_diacritics(word)\n",
    "        if undiacritized_word == word:\n",
    "            continue\n",
    "\n",
    "        # Add the undiacritized word to the dictionary\n",
    "        # If the undiacritized word is not in the dictionary, create a new entry\n",
    "        # Otherwise, update the existing entry with the diacritized word\n",
    "        if undiacritized_word not in word_dict:\n",
    "            word_dict[undiacritized_word] = {word}\n",
    "        else:\n",
    "            word_dict[undiacritized_word].add(word)\n",
    "\n",
    "    return word_dict, unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closestWords(possibilities_set, misspelled_word):\n",
    "    '''\n",
    "    This function returns a list of the closest words to the misspelled word\n",
    "    '''\n",
    "    # check the minimum edit distance between the misspelled word and the possibilities\n",
    "    min_distance = float(\"inf\")\n",
    "    closest_words = []\n",
    "\n",
    "    for word in possibilities_set:\n",
    "        distance = edit_distance(misspelled_word[:-1], word[:-1])\n",
    "\n",
    "        if distance < min_distance:\n",
    "            # Found a new minimum distance, update the list of closest words\n",
    "            min_distance = distance\n",
    "            closest_words = [word]\n",
    "        elif distance == min_distance:\n",
    "            # Found a tie, add this word to the list of closest words\n",
    "            closest_words.append(word)\n",
    "\n",
    "    return closest_words\n",
    "\n",
    "def wordEndings(word):\n",
    "    '''\n",
    "    This function returns the number of diacritics on the last letter of the word\n",
    "    '''\n",
    "    count = 0\n",
    "    if bool(re.match(r\"[\\u064b-\\u0652]{2}\", word[-2:])):\n",
    "        count = 2\n",
    "    elif bool(re.match(r\"[\\u064b-\\u0652]\", word[-1])):\n",
    "        count = 1\n",
    "    return count\n",
    "\n",
    "def matchLastDiacritic(closest_word, misspelled_word):\n",
    "    '''\n",
    "    This function matches the last diacritic of the misspelled word with the closest word\n",
    "    '''\n",
    "    count_endings_misspelled = wordEndings(misspelled_word)\n",
    "    count_endings_closest = wordEndings(closest_word)\n",
    "    # if both words have no diacritics on the last letter, return the closest word\n",
    "    if count_endings_misspelled == 0 and count_endings_closest == 0:\n",
    "        most_probable_word = closest_word\n",
    "    \n",
    "    # if the misspelled word has no diacritics on the last letter, return the closest word with the last diacritic of the misspelled word\n",
    "    elif count_endings_misspelled == 0:\n",
    "        most_probable_word = closest_word[:-count_endings_closest]\n",
    "    \n",
    "    # if the closest word has no diacritics on the last letter, return the closest word with the last diacritic of the misspelled word\n",
    "    elif count_endings_closest == 0:\n",
    "        most_probable_word = closest_word + misspelled_word[-count_endings_misspelled:]\n",
    "    \n",
    "    # if both words have diacritics on the last letter, return the closest word with the last diacritic of the misspelled word\n",
    "    else:\n",
    "        most_probable_word = closest_word[:-count_endings_closest] + misspelled_word[-count_endings_misspelled:]\n",
    "    return most_probable_word\n",
    "\n",
    "def postProcessDict(word_dict, unigram, misspelled_word):\n",
    "    '''\n",
    "    This function takes a dictionary of undiacritized words as keys and all possible diacritized words as values\n",
    "    in addition to a unigram frequency distribution of the words in the file\n",
    "    and a misspelled word as input and returns the most probable diacritized word\n",
    "    '''\n",
    "    undiacritized = remove_diacritics(misspelled_word)\n",
    "    if undiacritized in word_dict:\n",
    "        possibilities_set = word_dict[undiacritized]\n",
    "        closest_words = closestWords(possibilities_set, misspelled_word)\n",
    "        if len(closest_words) == 1:\n",
    "            return matchLastDiacritic(closest_words[0], misspelled_word)\n",
    "        else:\n",
    "            most_frequent_word = max(closest_words, key=lambda word: unigram[word])\n",
    "            if sum(1 for word in closest_words if unigram[word] == unigram[most_frequent_word]) > 1:\n",
    "                most_frequent_word = random.choice([word for word in closest_words if unigram[word] == unigram[most_frequent_word]])\n",
    "            return matchLastDiacritic(most_frequent_word, misspelled_word)\n",
    "    else:\n",
    "        return misspelled_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict, unigram = createDictionary('../data/train.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\".\\utilities\\POST_PROCESSING2.pickle\", 'wb') as pickle_file:\n",
    "    # Dump both dictionaries into the pickle file\n",
    "    pkl.dump((word_dict, unigram), pickle_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'تُقَدِّمْهَا', 'تُقَدِّمُهَا', 'تَقَدُّمُهَا', 'تَقَدُّمِهَا', 'تَقَدّمَهَا', 'تَقَدَّمَهَا', 'تُقَدَّمُهَا', 'تَقَدُّمَهَا'}\n",
      "['تَقَدّمَهَا']\n",
      "Misspelled word: تقدمها\n",
      "Corrected word: تَقَدّمَهَا\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "misspelled_word = \"تقدمها\"\n",
    "corrected_word = postProcessDict(word_dict, unigram, misspelled_word)\n",
    "print(\"Misspelled word:\", misspelled_word)\n",
    "print(\"Corrected word:\", corrected_word)\n",
    "print(edit_distance(misspelled_word, corrected_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post processing val_inference.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('val_inference1.txt', 'r', encoding='utf-8') as file:\n",
    "    lines = file.read()\n",
    "words = lines.split(' ')\n",
    "\n",
    "for word in words:\n",
    "    corrected_word = postProcessDict(word_dict, unigram, word)\n",
    "    with open('val_inference1_postprocessed.txt', 'a', encoding='utf-8') as file:\n",
    "        file.write(''.join(corrected_word))\n",
    "        file.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "تقدموا': {'تَقْدَمُوا', 'تَقَدَّمُوا', 'تُقَدِّمُوا'},\n"
     ]
    }
   ],
   "source": [
    "print(\"تقدموا': {'تَقْدَمُوا', 'تَقَدَّمُوا', 'تُقَدِّمُوا'},\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_misclassified_words(true_text, predicted_text):\n",
    "    true_words = true_text.split()\n",
    "    predicted_words = predicted_text.split()\n",
    "\n",
    "    highlighted_output = []\n",
    "    mis = 0\n",
    "\n",
    "    for line_number, (true_word, predicted_word) in enumerate(zip(true_words, predicted_words), start=1):\n",
    "        distance = edit_distance(true_word, predicted_word)\n",
    "        if distance > 0:\n",
    "            # Mark the misclassified word along with the line number\n",
    "            highlighted_output.append(f\"{true_word} [misclassified as: {predicted_word}]\\n\")\n",
    "            mis += distance\n",
    "        else:\n",
    "            highlighted_output.append(f\"{true_word}\")\n",
    "        \n",
    "\n",
    "    return ' '.join(highlighted_output), mis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DER before postprocessing:  1.2106096934406576\n",
      "DER after postprocessing :  1.0816278472341154\n"
     ]
    }
   ],
   "source": [
    "# Read true diacritized text from val.txt\n",
    "with open('val.txt', 'r', encoding='utf-8') as true_file:\n",
    "    true_diacritized_text = true_file.read()\n",
    "\n",
    "with open('val_inference1.txt', 'r', encoding='utf-8') as predicted_file:\n",
    "    predicted_diacritized_text = predicted_file.read()\n",
    "\n",
    "# Highlight misclassified words with line numbers\n",
    "highlighted_output, mis1 = highlight_misclassified_words(true_diacritized_text, predicted_diacritized_text)\n",
    "\n",
    "# Read predicted diacritized text from val_inference.txt\n",
    "with open('val_inference1_postprocessed.txt', 'r', encoding='utf-8') as predicted_file:\n",
    "    predicted_diacritized_text = predicted_file.read()\n",
    "\n",
    "# Highlight misclassified words with line numbers\n",
    "highlighted_output, mis2 = highlight_misclassified_words(true_diacritized_text, predicted_diacritized_text)\n",
    "\n",
    "# Write the highlighted output to a file\n",
    "with open('misclassified_words_output.txt', 'w', encoding='utf-8') as output_file:\n",
    "    output_file.write(highlighted_output)\n",
    "\n",
    "\n",
    "print(\"DER before postprocessing: \", mis1*100/len(true_diacritized_text))\n",
    "print(\"DER after postprocessing : \", mis2*100/len(true_diacritized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_misclassified_words(true_text, predicted_text):\n",
    "    true_words = true_text.split()\n",
    "    predicted_words = predicted_text.split()\n",
    "\n",
    "    highlighted_output = []\n",
    "\n",
    "    for true_word, predicted_word in zip(true_words, predicted_words):\n",
    "        if true_word != predicted_word:\n",
    "            # Check if only the last letter is misclassified\n",
    "            if len(true_word) > 1 and true_word[:-1] == predicted_word[:-1]:\n",
    "                # Exclude words where only the last letter is misclassified\n",
    "                highlighted_output.append(true_word)\n",
    "            else:\n",
    "                # Mark the misclassified word\n",
    "                highlighted_output.append(f\"{true_word} [misclassified as: {predicted_word}]\\n\")\n",
    "        else:\n",
    "            highlighted_output.append(true_word)\n",
    "\n",
    "    return ' '.join(highlighted_output)\n",
    "\n",
    "# Read true diacritized text from val.txt\n",
    "with open('val.txt', 'r', encoding=\"utf-8\") as true_file:\n",
    "    true_diacritized_text = true_file.read()\n",
    "\n",
    "# Read predicted diacritized text from val_inference.txt\n",
    "with open('val_inference1_postprocessed.txt', 'r', encoding=\"utf-8\") as predicted_file:\n",
    "    predicted_diacritized_text = predicted_file.read()\n",
    "\n",
    "# Highlight misclassified words\n",
    "highlighted_output = highlight_misclassified_words(true_diacritized_text, predicted_diacritized_text)\n",
    "\n",
    "# Write the highlighted output to a file\n",
    "with open('misclassified_words_output.txt', 'w', encoding=\"utf-8\") as output_file:\n",
    "    output_file.write(highlighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"وَكَانَ الطَّالِبُ الجميل الرائع العظيم عُمَرُ يَلْهُو فِي الْفَصْلِ\"\n",
    "\n",
    "def add_shadda_to_lam_shamsia(text):\n",
    "    # Regex pattern to match Lam Shamsia followed by specific letters without shadda\n",
    "    \n",
    "    pattern = re.compile(r'(?<=ال)[تثدذرزسشصضطظلن](?!(\\u0651|\\u064e\\u0651|\\u064f\\u0651|\\u0650\\u0651))')\n",
    "\n",
    "    # Function to add shadda to the match\n",
    "    def add_shadda(match):\n",
    "        return match.group(0) + 'ّ'\n",
    "\n",
    "    # Apply the regex and replacement\n",
    "    result = pattern.sub(add_shadda, text)\n",
    "\n",
    "    return result\n",
    "\n",
    "def add_sukoon_to_lam_qamaria(text):\n",
    "    # Regex pattern to match Lam Qamaria followed by specific letters without sukoon\n",
    "    pattern = re.compile(r'(?<=ال)(?=[أآإبجحخعغفقكمهوي])')\n",
    "\n",
    "    # Function to add sukoon to the match\n",
    "    def add_sukoon(match):\n",
    "        return match.group(0) + 'ْ'\n",
    "\n",
    "    # Apply the regex and replacement\n",
    "    result = pattern.sub(add_sukoon, text)\n",
    "\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "arabic_text = \"الظابط الولد الطَّالِبُ الجميل الرَُائع العظيم عُمَرُ يَلْهُو فِي الْفَصْلِ\"\n",
    "text_with_shadda = add_shadda_to_lam_shamsia(arabic_text)\n",
    "\n",
    "with open('outputs.txt', '+w', encoding='utf-8') as file:\n",
    "    file.write(''.join(arabic_text))\n",
    "    file.write('\\n')\n",
    "    file.write('\\n')\n",
    "    file.write(''.join(text_with_shadda))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
