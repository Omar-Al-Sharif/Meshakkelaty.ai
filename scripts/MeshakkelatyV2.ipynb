{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import pickle as pkl\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class TashkeelDataset(Dataset):\n",
    "    def __init__(self, name, path):\n",
    "        self.name = name\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            self.lines = list(tqdm(file, f\"Reading {self.name} Lines\"))\n",
    "        self._load_dicts()\n",
    "        self.tokenized_lines = self._tokenize_lines()\n",
    "        self.embedded_data = self._embedd_lines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embedded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.embedded_data[idx]\n",
    "        return torch.tensor(x).to(device), torch.tensor(y).to(device)\n",
    "    \n",
    "    def _remove_tashkeel(self,data):\n",
    "        #double damma, double fatha, double kasera, damma, fatha, kasera, sukoon, shadd\n",
    "        TASHKEEL_SET = {'ٌ', 'ً', 'ٍ', 'ُ', 'َ', 'ِ', 'ْ', 'ٌّ', 'ّ'}\n",
    "        DIACRITICS_REGEX = re.compile('|'.join(TASHKEEL_SET))\n",
    "        return re.sub(DIACRITICS_REGEX, '', data)\n",
    "    \n",
    "    def _one_hot_encode(self, indices, size):\n",
    "        return [[1 if i == elem else 0 for i in range(size)] for elem in indices]\n",
    "    \n",
    "    def _chunk_text(self, text, chunk_size):\n",
    "        chunks = []\n",
    "        words = re.findall(r'\\S+', text)\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        for word in words:\n",
    "            if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
    "                current_chunk += f\"{word} \"\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = f\"{word} \"\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return list(filter(None, chunks))\n",
    "    \n",
    "    def _tokenize_lines(self):\n",
    "        # Define a pattern to match specific punctuation marks\n",
    "        punctuation_pattern1 = r'([.,:;؛)\\]}»،])'\n",
    "        punctuation_pattern2 = r'([(\\[{«])'\n",
    "        tokenized_lines = []\n",
    "\n",
    "        for line in tqdm(self.lines, f\"Tokenizing {self.name} Lines\"):\n",
    "            # Replace matched punctuation marks with the same followed by a line break\n",
    "            splitted_line = re.sub(punctuation_pattern1, r'\\1\\n', line)\n",
    "            splitted_line = re.sub(punctuation_pattern2, r'\\n\\1', splitted_line)\n",
    "\n",
    "            # Further split the splitted line into substrings based on line breaks\n",
    "            for sub_line in splitted_line.split('\\n'):\n",
    "                cleaned_sub_line = self._remove_tashkeel(sub_line).strip()\n",
    "                if 0 < len(cleaned_sub_line) <= 500:\n",
    "                    tokenized_lines.append(sub_line.strip())\n",
    "\n",
    "                elif len(cleaned_sub_line) > 500:\n",
    "                    tokenized_lines.extend(self._chunk_text(sub_line.strip(), 500))\n",
    "    \n",
    "        return tokenized_lines\n",
    "\n",
    "    def _load_dicts(self):\n",
    "        with open( '../utilities/pickle_files/LETTERS.pickle', 'rb') as file:\n",
    "            self.LETTERS = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/DIACRITICS.pickle', 'rb') as file:\n",
    "            self.DIACRITICS = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/CHAR_TO_ID.pickle', 'rb') as file:\n",
    "            self.CHAR_TO_ID = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/DIACRITIC_TO_ID.pickle', 'rb') as file:\n",
    "            self.DIACRITIC_TO_ID = pkl.load(file)\n",
    "        \n",
    "    def _embedd_lines(self):\n",
    "        inputs_embeddings=[]\n",
    "        for line in tqdm(self.tokenized_lines, f\"Embedding {self.name} Lines\"):\n",
    "            x = [self.CHAR_TO_ID['<SOS>']]\n",
    "            y = [self.DIACRITIC_TO_ID['<SOS>']]\n",
    "\n",
    "            for index, char in enumerate(line):\n",
    "                if char in self.DIACRITICS:\n",
    "                    continue\n",
    "\n",
    "                if char in self.CHAR_TO_ID:\n",
    "                    x.append(self.CHAR_TO_ID[char])\n",
    "                else:\n",
    "                    x.append(self.CHAR_TO_ID['<UNK>'])\n",
    "\n",
    "                if char not in self.LETTERS:\n",
    "                    y.append(self.DIACRITIC_TO_ID[''])\n",
    "\n",
    "                else:\n",
    "                    char_diac = ''\n",
    "                    if index + 1 < len(line) and line[index + 1] in self.DIACRITICS:\n",
    "                        char_diac = line[index + 1]\n",
    "                        if index + 2 < len(line) and line[index + 2] in self.DIACRITICS and char_diac + line[index + 2] in self.DIACRITIC_TO_ID:\n",
    "                            char_diac += line[index + 2]\n",
    "                        elif index + 2 < len(line) and line[index + 2] in self.DIACRITICS and line[index + 2] + char_diac in self.DIACRITIC_TO_ID:\n",
    "                            char_diac = line[index + 2] + char_diac\n",
    "                    y.append(self.DIACRITIC_TO_ID[char_diac])\n",
    "\n",
    "            x.append(self.CHAR_TO_ID['<EOS>'])\n",
    "            y.append(self.DIACRITIC_TO_ID['<EOS>'])\n",
    "            y = self._one_hot_encode(y, len(self.DIACRITIC_TO_ID))\n",
    "            \n",
    "            inputs_embeddings.append((x, y)) \n",
    "            \n",
    "        return inputs_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train dataset Lines: 2500it [00:00, 208385.70it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset Lines: 100%|██████████| 2500/2500 [00:00<00:00, 12685.87it/s]\n",
      "Embedding train dataset Lines: 100%|██████████| 15362/15362 [00:01<00:00, 8374.72it/s]\n",
      "Reading validation dataset Lines: 2500it [00:00, 250083.71it/s]\n",
      "Tokenizing validation dataset Lines: 100%|██████████| 2500/2500 [00:00<00:00, 11955.14it/s]\n",
      "Embedding validation dataset Lines: 100%|██████████| 15362/15362 [00:02<00:00, 6805.47it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TashkeelDataset('train dataset','../data/val.txt')\n",
    "val_dataset = TashkeelDataset('validation dataset','../data/val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    x_padded = rnn_utils.pad_sequence(x_batch, batch_first=True, padding_value=train_dataset.CHAR_TO_ID['<PAD>'])\n",
    "    y_padded = rnn_utils.pad_sequence(y_batch, batch_first=True, padding_value=train_dataset.DIACRITIC_TO_ID['<PAD>'])\n",
    "    return x_padded, y_padded\n",
    "\n",
    "# Create a DataLoader instance with collate_fn\n",
    "dataloader_train = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "dataloader_test = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class MeshakkelatyModel(nn.Module):\n",
    "    def __init__(self, char_to_id, diacritic_to_id):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=len(char_to_id),\n",
    "            embedding_dim=25,\n",
    "            padding_idx=char_to_id['<PAD>']  \n",
    "        )\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=25,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            dropout=0.5,\n",
    "            batch_first=True  \n",
    "        )\n",
    "        self.linear1 = nn.Linear(2*256, 512)\n",
    "        self.linear2 = nn.Linear(512, len(diacritic_to_id))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = nn.functional.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "meshakkelaty = MeshakkelatyModel(train_dataset.CHAR_TO_ID, train_dataset.DIACRITIC_TO_ID).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(meshakkelaty.parameters())\n",
    "epochs = 10\n",
    "metric = Accuracy(task=\"multiclass\", num_classes=len(train_dataset.DIACRITIC_TO_ID)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    meshakkelaty.train()\n",
    "\n",
    "    # Initialize variables to accumulate correct and total predictions\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    epoch_progress = tqdm(dataloader_train, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    for x_batch, y_batch in epoch_progress:\n",
    "    \n",
    "        x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = meshakkelaty(x_batch)\n",
    "        loss = criterion(y_pred, y_batch.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Convert one-hot encoded predictions and targets to class indices\n",
    "        y_pred_class = y_pred.argmax(dim=-1)\n",
    "        y_batch_class = y_batch.argmax(dim=-1)\n",
    "        train_acc = metric(y_pred_class, y_batch_class)\n",
    "\n",
    "        # Update accumulated values\n",
    "        total_correct += torch.sum(y_pred_class == y_batch_class).item()\n",
    "\n",
    "        total_samples += y_batch.size(0) * y_batch.size(1)\n",
    "\n",
    "        # Calculate accuracy for the current batch\n",
    "        batch_acc = total_correct / total_samples\n",
    "\n",
    "        # Update the progress bar description with the current accuracy\n",
    "        epoch_progress.set_description(f\"Epoch {epoch + 1}/{epochs}, Train Accuracy: {metric.compute()*100:.4f}%\", refresh=True)\n",
    "        # total_correct = 0 \n",
    "    # Print a newline to move to the next line after the epoch is finished\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train Accuracy: {metric.compute()*100:.4f}%')\n",
    "    metric.reset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
