{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm\n",
    "\n",
    "class TashkeelDataset(Dataset):\n",
    "    def __init__(self, name, path):\n",
    "        self.name = name\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            self.lines = list(tqdm(file, f\"Reading {self.name} Lines\"))\n",
    "        self._load_dicts()\n",
    "        self.tokenized_lines = self._tokenize_lines()\n",
    "        self.embedded_data = self._embedd_lines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embedded_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.embedded_data[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "    \n",
    "    def _remove_tashkeel(self,data):\n",
    "        #double damma, double fatha, double kasera, damma, fatha, kasera, sukoon, shadd\n",
    "        TASHKEEL_SET = {'ٌ', 'ً', 'ٍ', 'ُ', 'َ', 'ِ', 'ْ', 'ٌّ', 'ّ'}\n",
    "        DIACRITICS_REGEX = re.compile('|'.join(TASHKEEL_SET))\n",
    "        return re.sub(DIACRITICS_REGEX, '', data)\n",
    "    \n",
    "    def _one_hot_encode(self, indices, size):\n",
    "        return [[1 if i == elem else 0 for i in range(size)] for elem in indices]\n",
    "\n",
    "    def _chunk_text(self, text, chunk_size):\n",
    "        chunks = []\n",
    "        words = re.findall(r'\\S+', text)\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        for word in words:\n",
    "            if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
    "                current_chunk += f\"{word} \"\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = f\"{word} \"\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return list(filter(None, chunks))\n",
    "    \n",
    "    def _tokenize_lines(self):\n",
    "        # Define a pattern to match specific punctuation marks\n",
    "        punctuation_pattern1 = r'([.,:;؛)\\]}»،])'\n",
    "        punctuation_pattern2 = r'([(\\[{«])'\n",
    "        tokenized_lines = []\n",
    "\n",
    "        for line in tqdm(self.lines, f\"Tokenizing {self.name} Lines\"):\n",
    "            # Replace matched punctuation marks with the same followed by a line break\n",
    "            splitted_line = re.sub(punctuation_pattern1, r'\\1\\n', line)\n",
    "            splitted_line = re.sub(punctuation_pattern2, r'\\n\\1', splitted_line)\n",
    "\n",
    "            # Further split the splitted line into substrings based on line breaks\n",
    "            for sub_line in splitted_line.split('\\n'):\n",
    "                cleaned_sub_line = self._remove_tashkeel(sub_line).strip()\n",
    "                if 0 < len(cleaned_sub_line) <= 500:\n",
    "                    tokenized_lines.append(sub_line.strip())\n",
    "\n",
    "                elif len(cleaned_sub_line) > 500:\n",
    "                    tokenized_lines.extend(self._chunk_text(sub_line.strip(), 500))\n",
    "    \n",
    "        return tokenized_lines\n",
    "\n",
    "    def _load_dicts(self):\n",
    "        with open( '../utilities/pickle_files/LETTERS.pickle', 'rb') as file:\n",
    "            self.LETTERS = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/DIACRITICS.pickle', 'rb') as file:\n",
    "            self.DIACRITICS = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/CHAR_TO_ID.pickle', 'rb') as file:\n",
    "            self.CHAR_TO_ID = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/DIACRITIC_TO_ID.pickle', 'rb') as file:\n",
    "            self.DIACRITIC_TO_ID = pkl.load(file)\n",
    "        # self.CHAR_TO_ID['<UNK>'] = len(self.CHAR_TO_ID)\n",
    "        \n",
    "    def _embedd_lines(self):\n",
    "        inputs_embeddings=[]\n",
    "        for line in tqdm(self.tokenized_lines, f\"Embedding {self.name} Lines\"):\n",
    "            x = [self.CHAR_TO_ID['<SOS>']]\n",
    "            y = [self.DIACRITIC_TO_ID['<SOS>']]\n",
    "\n",
    "            for index, char in enumerate(line):\n",
    "                if char in self.CHAR_TO_ID:\n",
    "                    x.append(self.CHAR_TO_ID[char])\n",
    "                else: \n",
    "                    x.append(self.CHAR_TO_ID['<UNK>'])\n",
    "\n",
    "                if char not in self.LETTERS:\n",
    "                    y.append(self.DIACRITIC_TO_ID[''])\n",
    "                else:\n",
    "                    char_diac = ''\n",
    "                    if index + 1 < len(line) and line[index + 1] in self.DIACRITICS:\n",
    "                        char_diac = line[index + 1]\n",
    "                        if index + 2 < len(line) and line[index + 2] in self.DIACRITICS and char_diac + line[index + 2] in self.DIACRITIC_TO_ID:\n",
    "                            char_diac += line[index + 2]\n",
    "                        elif index + 2 < len(line) and line[index + 2] in self.DIACRITICS and line[index + 2] + char_diac in self.DIACRITIC_TO_ID:\n",
    "                            char_diac = line[index + 2] + char_diac\n",
    "                    y.append(self.DIACRITIC_TO_ID[char_diac])\n",
    "\n",
    "            x.append(self.CHAR_TO_ID['<EOS>'])\n",
    "            y.append(self.DIACRITIC_TO_ID['<EOS>'])\n",
    "            y = self._one_hot_encode(y, len(self.DIACRITIC_TO_ID))\n",
    "            \n",
    "            inputs_embeddings.append((x, y)) \n",
    "            \n",
    "        return inputs_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading train dataset Lines: 2500it [00:00, 119088.70it/s]\n",
      "Tokenizing train dataset Lines: 100%|██████████| 2500/2500 [00:00<00:00, 12146.25it/s]\n",
      "Embedding train dataset Lines: 100%|██████████| 15362/15362 [00:01<00:00, 7789.69it/s]\n",
      "Reading validation dataset Lines: 2500it [00:00, 206966.68it/s]\n",
      "Tokenizing validation dataset Lines: 100%|██████████| 2500/2500 [00:00<00:00, 12019.44it/s]\n",
      "Embedding validation dataset Lines: 100%|██████████| 15362/15362 [00:02<00:00, 7467.63it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = TashkeelDataset('train dataset','../data/val.txt')\n",
    "val_dataset = TashkeelDataset('validation dataset','../data/val.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    x_padded = rnn_utils.pad_sequence(x_batch, batch_first=True, padding_value=train_dataset.CHAR_TO_ID['<PAD>'])\n",
    "    y_padded = rnn_utils.pad_sequence(y_batch, batch_first=True, padding_value=train_dataset.DIACRITIC_TO_ID['<PAD>'])\n",
    "    return x_padded, y_padded\n",
    "\n",
    "# Create a DataLoader instance with collate_fn\n",
    "dataloader_train = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "dataloader_test = DataLoader(val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class MeshakkelatyModel(nn.Module):\n",
    "    def __init__(self, char_to_id, diacritic_to_id):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=len(char_to_id),\n",
    "            embedding_dim=25,\n",
    "            padding_idx=char_to_id['<PAD>']  \n",
    "        )\n",
    "        self.lstm1 = nn.LSTM(\n",
    "            input_size=25,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            dropout=0.5,\n",
    "            batch_first=True  \n",
    "        )\n",
    "        self.linear1 = nn.Linear(2*256, 512)\n",
    "        self.linear2 = nn.Linear(512, len(diacritic_to_id))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = nn.functional.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshakkelaty = MeshakkelatyModel(train_dataset.CHAR_TO_ID, train_dataset.DIACRITIC_TO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(meshakkelaty.parameters())\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[192], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m meshakkelaty(x_batch)\n\u001b[0;32m      6\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(y_pred\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_dataset\u001b[38;5;241m.\u001b[39mDIACRITIC_TO_ID)), y_batch\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     10\u001b[0m meshakkelaty\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\Omar\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Omar\\anaconda3\\envs\\nlp\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    meshakkelaty.train()\n",
    "    for x_batch, y_batch in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = meshakkelaty(x_batch)\n",
    "        loss = criterion(y_pred.view(-1, len(train_dataset.DIACRITIC_TO_ID)), y_batch.argmax(dim=-1).view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    meshakkelaty.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in dataloader_test:\n",
    "            y_val_pred = meshakkelaty(x_val)\n",
    "            val_loss += criterion(y_val_pred.view(-1, len(train_dataset.DIACRITIC_TO_ID)), y_val.argmax(dim=-1).view(-1)).item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss.item():.4f}, Val Loss: {val_loss / len(dataloader_test):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
