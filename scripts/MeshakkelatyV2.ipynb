{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import re\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "\n",
    "class TashkeelDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        with open(path, 'r', encoding='utf-8') as file:\n",
    "            self.lines = lines = file.readlines()\n",
    "        self._load_dicts()\n",
    "        self.tokenized_lines = self._tokenize_lines()\n",
    "        self.embedded_data = self._embedd_lines()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.embedded_data[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "    \n",
    "    def _remove_tashkeel(self,data):\n",
    "        #double damma, double fatha, double kasera, damma, fatha, kasera, sukoon, shadd\n",
    "        TASHKEEL_SET = {'ٌ', 'ً', 'ٍ', 'ُ', 'َ', 'ِ', 'ْ', 'ٌّ', 'ّ'}\n",
    "        DIACRITICS_REGEX = re.compile('|'.join(TASHKEEL_SET))\n",
    "        return re.sub(DIACRITICS_REGEX, '', data)\n",
    "    \n",
    "    def _one_hot_encode(self, indices, size):\n",
    "        return [[1 if i == elem else 0 for i in range(size)] for elem in indices]\n",
    "\n",
    "    def _chunk_text(self, text, chunk_size):\n",
    "        chunks = []\n",
    "        words = re.findall(r'\\S+', text)\n",
    "\n",
    "        current_chunk = \"\"\n",
    "        for word in words:\n",
    "            if len(current_chunk) + len(word) + 1 <= chunk_size:\n",
    "                current_chunk += f\"{word} \"\n",
    "            else:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = f\"{word} \"\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return list(filter(None, chunks))\n",
    "    \n",
    "    def _tokenize_lines(self):\n",
    "        # Define a pattern to match specific punctuation marks\n",
    "        punctuation_pattern1 = r'([.,:;؛)\\]}»،])'\n",
    "        punctuation_pattern2 = r'([(\\[{«])'\n",
    "        tokenized_lines = []\n",
    "\n",
    "        for line in self.lines:\n",
    "            # Replace matched punctuation marks with the same followed by a line break\n",
    "            splitted_line = re.sub(punctuation_pattern1, r'\\1\\n', line)\n",
    "            splitted_line = re.sub(punctuation_pattern2, r'\\n\\1', splitted_line)\n",
    "\n",
    "            # Further split the splitted line into substrings based on line breaks\n",
    "            for sub_line in splitted_line.split('\\n'):\n",
    "                cleaned_sub_line = self._remove_tashkeel(sub_line).strip()\n",
    "                if 0 < len(cleaned_sub_line) <= 500:\n",
    "                    tokenized_lines.append(sub_line.strip())\n",
    "\n",
    "                elif len(cleaned_sub_line) > 500:\n",
    "                    tokenized_lines.extend(self._chunk_text(sub_line.strip(), 500))\n",
    "    \n",
    "        return tokenized_lines\n",
    "\n",
    "    def _load_dicts(self):\n",
    "        with open( '../utilities/pickle_files/LETTERS.pickle', 'rb') as file:\n",
    "            self.LETTERS = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/DIACRITICS.pickle', 'rb') as file:\n",
    "            self.DIACRITICS = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/CHAR_TO_ID.pickle', 'rb') as file:\n",
    "            self.CHAR_TO_ID = pkl.load(file)\n",
    "        with open( '../utilities/pickle_files/DIACRITIC_TO_ID.pickle', 'rb') as file:\n",
    "            self.DIACRITIC_TO_ID = pkl.load(file)\n",
    "        self.CHAR_TO_ID['<UNK>'] = len(self.CHAR_TO_ID)\n",
    "        \n",
    "    def _embedd_lines(self):\n",
    "        inputs_embeddings=[]\n",
    "        one_hot_labels=[]\n",
    "        for line in self.tokenized_lines:\n",
    "            x = [self.CHAR_TO_ID['<SOS>']]\n",
    "            y = [self.DIACRITIC_TO_ID['<SOS>']]\n",
    "\n",
    "            for index, char in enumerate(line):\n",
    "                if char in self.CHAR_TO_ID:\n",
    "                    x.append(self.CHAR_TO_ID[char])\n",
    "                else: \n",
    "                    x.append(self.CHAR_TO_ID['<UNK>'])\n",
    "\n",
    "                if char not in self.LETTERS:\n",
    "                    y.append(self.DIACRITIC_TO_ID[''])\n",
    "                else:\n",
    "                    char_diac = ''\n",
    "                    if index + 1 < len(line) and line[index + 1] in self.DIACRITICS:\n",
    "                        char_diac = line[index + 1]\n",
    "                        if index + 2 < len(line) and line[index + 2] in self.DIACRITICS and char_diac + line[index + 2] in self.DIACRITIC_TO_ID:\n",
    "                            char_diac += line[index + 2]\n",
    "                        elif index + 2 < len(line) and line[index + 2] in self.DIACRITICS and line[index + 2] + char_diac in self.DIACRITIC_TO_ID:\n",
    "                            char_diac = line[index + 2] + char_diac\n",
    "                    y.append(self.DIACRITIC_TO_ID[char_diac])\n",
    "\n",
    "            x.append(self.CHAR_TO_ID['<EOS>'])\n",
    "            y.append(self.DIACRITIC_TO_ID['<EOS>'])\n",
    "            y = self._one_hot_encode(y, len(self.DIACRITIC_TO_ID))\n",
    "            \n",
    "            inputs_embeddings.append(x)\n",
    "            one_hot_labels.append(y)\n",
    "\n",
    "        return inputs_embeddings, one_hot_labels\n",
    "    \n",
    "    def _pad_sequences(self, sequences, pad_token_x, pad_token_y):\n",
    "        max_len_x = max(len(seq[0]) for seq in sequences)\n",
    "        max_len_y = max(len(seq[1]) for seq in sequences)\n",
    "\n",
    "        padded_sequences_x = [seq[0] + [pad_token_x] * (max_len_x - len(seq[0])) for seq in sequences]\n",
    "        padded_sequences_y = [seq[1] + [pad_token_y] * (max_len_y - len(seq[1])) for seq in sequences]\n",
    "\n",
    "        return padded_sequences_x, padded_sequences_y\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "\n",
    "        # Padding sequences with different tokens\n",
    "        x_padded, y_padded = self._pad_sequences(list(zip(x_batch, y_batch)), self.CHAR_TO_ID['<PAD>'], self.DIACRITIC_TO_ID['<PAD>'])\n",
    "\n",
    "        return torch.tensor(x_padded), torch.tensor(y_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TashkeelDataset('../data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader instance with collate_fn\n",
    "dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True, collate_fn=TashkeelDataset.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class MeshakkelatyModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
    "        super(MeshakkelatyModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.blstm1 = nn.LSTM(embedding_dim, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.blstm2 = nn.LSTM(2 * hidden_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dense1 = nn.Linear(2 * hidden_size, 512)\n",
    "        self.dense2 = nn.Linear(512, 512)\n",
    "        self.output = nn.Linear(512, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.blstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.blstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meshakkelaty = MeshakkelatyModel(vocab_size=len(train_dataset.CHAR_TO_ID), embedding_dim=25, hidden_size=256, output_size=len(train_dataset.DIACRITIC_TO_ID))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
